{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XMQiayAqdQ1"
      },
      "source": [
        "# 0. AI 작사가 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoAkUFdTqdQ3"
      },
      "source": [
        "AI 작사가를 만들어보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTrK1vpTqdQ4"
      },
      "source": [
        "## 0.1. 필요한 라이브러리 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js2rNPKPMJcu",
        "outputId": "4a871a8c-9fe9-498a-96b0-65073df4b2a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 17924222384655484643\n",
            "xla_global_id: -1\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import os, re\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml38XgXmMJcx"
      },
      "source": [
        "# 1. 데이터 읽어오기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9oPy0gYqdQ6"
      },
      "source": [
        "여러 노래의 가사가 들어 있는 txt파일 49개를 전부 불러온다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQgZ3FWEMKRM",
        "outputId": "2fd1089c-e349-4597-9fbf-f6a6dcc25a23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/AIFFEL/Exploration/6/data/*\n"
          ]
        }
      ],
      "source": [
        "# colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "txt_file_path = '/content/drive/MyDrive/AIFFEL/Exploration/6/data/*'\n",
        "\n",
        "# local\n",
        "# txt_file_path = os.getenv('USERPROFILE') + '\\OneDrive - 수원대학교\\Office\\AIFFEL\\Exploration\\\\6\\data\\*'\n",
        "\n",
        "print(txt_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3m7UYDVfMJcy",
        "outputId": "c12e6ddb-3acd-4cae-b91c-fd959dc17878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/AIFFEL/Exploration/6/data/r-kelly.txt', '/content/drive/MyDrive/AIFFEL/Exploration/6/data/radiohead.txt', '/content/drive/MyDrive/AIFFEL/Exploration/6/data/paul-simon.txt', '/content/drive/MyDrive/AIFFEL/Exploration/6/data/patti-smith.txt', '/content/drive/MyDrive/AIFFEL/Exploration/6/data/nursery_rhymes.txt']\n"
          ]
        }
      ],
      "source": [
        "txt_list = glob.glob(txt_file_path) # 해당 경로에 있는 모든 파일들의 절대경로를 리스트에 저장.\n",
        "print(txt_list[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmeOnXrdqdQ7",
        "outputId": "ca677bbd-d44c-4d61-cc60-e2288c54fa19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['I hear you callin\\', \"Here I come baby\"', 'To save you, oh oh', \"Baby no more stallin'\"]\n"
          ]
        }
      ],
      "source": [
        "raw_corpus = []\n",
        "\n",
        "for txt_file in txt_list: # 파일경로를 하나씩\n",
        "    with open(txt_file, \"r\", encoding='UTF8') as f: # 읽어 오는데\n",
        "        sentences = f.read().splitlines() # 줄바꿈 기준으로 자른 후(1라인=1문장)\n",
        "        raw_corpus.extend(sentences) # raw_corpus에 unpacking해서 저장(49개 파일의 모든 문장이 저장됨.)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njD74SPvMJcz",
        "outputId": "9a99323a-69f8-4430-c073-d8752c2be8fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I hear you callin', \"Here I come baby\"\n",
            "To save you, oh oh\n",
            "Baby no more stallin'\n",
            "These hands have been longing to touch you baby\n",
            "And now that you've come around, to seein' it my way\n",
            "You won't regret it baby, and you surely won't forget it baby\n",
            "It's unbelieveable how your body's calling for me\n",
            "I can just hear it callin' callin' for me My body's callin' for you\n",
            "My body's callin' for you\n",
            "My body's callin' for you\n"
          ]
        }
      ],
      "source": [
        "# 문장 10개 둘러보기\n",
        "for idx, sentence in enumerate(raw_corpus):\n",
        "    if len(sentence) == 0: continue\n",
        "    if idx > 9: break\n",
        "        \n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0Q_p0p7MJc0"
      },
      "source": [
        "# 2. 데이터 정제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SJK3tOswMJc0"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(sentence): # 토큰화를 하기 전 문장 정제\n",
        "    sentence = sentence.lower().strip() # 전체 소문자로 변경 후 양 끝 띄어쓰기 제거\n",
        "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특정 특수문자 양쪽에 띄어쓰기 생성\n",
        "    sentence = re.sub(r'[ ]+', \" \", sentence) # 중복된 띄어쓰기를 하나로 변경\n",
        "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 소문자, 대문자, 특정 특수문자가 아닌 것들(^ 명령어)을 띄어쓰기로 변경\n",
        "    sentence = sentence.strip() # 다시 또 양 끝 띄어쓰기 제거\n",
        "    sentence = '<start> ' + sentence + ' <end>'\n",
        "    \n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp5WJYOrMJc1",
        "outputId": "8b00f024-7402-4358-be77-de89db578399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<start> this is sample sentence . <end>\n"
          ]
        }
      ],
      "source": [
        "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\")) # 잘 작동되는지 확인."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pQV3eodMJc1",
        "outputId": "58d49c78-448c-42bf-ef22-9a79d188f8d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<start> i hear you callin , here i come baby <end>',\n",
              " '<start> to save you , oh oh <end>',\n",
              " '<start> baby no more stallin <end>',\n",
              " '<start> these hands have been longing to touch you baby <end>',\n",
              " '<start> and now that you ve come around , to seein it my way <end>',\n",
              " '<start> it s unbelieveable how your body s calling for me <end>',\n",
              " '<start> my body s callin for you <end>',\n",
              " '<start> my body s callin for you <end>',\n",
              " '<start> my body s callin for you tell me , what s your desire <end>',\n",
              " '<start> baby your wish is my deal oh yes it is baby <end>']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    if len(sentence) == 0: continue # 빈 문자열 제거\n",
        "        \n",
        "    preprocessed_sentence = preprocess_sentence(sentence) # 정제\n",
        "    if len(preprocessed_sentence.split()) <= 15:  # 토큰의 개수가 15개 이하인 문장들만.\n",
        "        corpus.append(preprocessed_sentence) # 정제된 문장 저장\n",
        "\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gomRpE0qdQ-"
      },
      "source": [
        "## 2.1 토큰화하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxxUWaeMqdQ-"
      },
      "source": [
        "너무 긴 문장은 다른 sequence들이 과도한 Padding을 갖게 하므로 제거한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0r1c5qxdIqOB"
      },
      "outputs": [],
      "source": [
        "def tokenize(corpus): # 토큰화(간단히 띄어쓰기 기준으로 split한 느낌이라고 보면 됨.)\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=12000, filters=' ', oov_token=\"<unk>\") # 많이 쓰인 중복 제외 단어 12000개만 인식하고 나머지는 <unk>로 토큰화.\n",
        "    tokenizer.fit_on_texts(corpus) # corpus 변수를 fitting. 단어와 인덱스를 대응시킨 딕셔너리가 생성됨.\n",
        "    sequences = tokenizer.texts_to_sequences(corpus) # corpus의 모든 단어들을 인덱스로 바꾼 시퀀스 형태로 변경.\n",
        "    sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences)  # 토큰이 15개 보다 적은 문장들에 패딩을 붙여줌.\n",
        "    print(sequences, tokenizer)\n",
        "\n",
        "    return sequences, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEsg5Ov3Iq1q",
        "outputId": "d2bd6adb-567a-4623-b8e3-73525307c665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ...   68   52    3]\n",
            " [   0    0    0 ...   47   47    3]\n",
            " [   0    0    0 ...   98 6829    3]\n",
            " ...\n",
            " [   0    0    2 ...   22 4255    3]\n",
            " [   0    0    0 ... 4255  244    3]\n",
            " [   0    0    0 ...    0    2    3]] <keras_preprocessing.text.Tokenizer object at 0x7faac7a241d0>\n"
          ]
        }
      ],
      "source": [
        "sequences, tokenizer = tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCxCCdLaqdQ-",
        "outputId": "673d006b-9944-40c1-a819-6ff08f48372a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   4  186    7  824    5   90    4   68   52    3]\n",
            " [   0    0    2   10  588    7    5   47   47    3]\n",
            " [   0    0    0    0    2   52   41   98 6829    3]]\n"
          ]
        }
      ],
      "source": [
        "print(sequences[:3, 5:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9we-dkmFMJc3",
        "outputId": "eb88b3b9-9b1b-4844-9893-de38e49dd7dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : <unk>\n",
            "2 : <start>\n",
            "3 : <end>\n",
            "4 : i\n",
            "5 : ,\n",
            "6 : the\n",
            "7 : you\n",
            "8 : and\n",
            "9 : a\n",
            "10 : to\n"
          ]
        }
      ],
      "source": [
        "# 가장 빈도가 높은 단어들 확인.\n",
        "for idx in tokenizer.index_word:\n",
        "    print(idx, \":\", tokenizer.index_word[idx])\n",
        "\n",
        "    if idx >= 10: break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RBx24dBMJc3",
        "outputId": "7f125025-3a1e-4076-ef3f-531b278d01ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   2   4 186   7 824   5  90   4  68  52]\n",
            "[  0   0   0   2   4 186   7 824   5  90   4  68  52   3]\n"
          ]
        }
      ],
      "source": [
        "# 입력 텐서 source, 출력 텐서 target 데이터 생성.\n",
        "source = sequences[:, :-1]\n",
        "target = sequences[:, 1:]\n",
        "\n",
        "print(source[0])\n",
        "print(target[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAQwSnnut51I"
      },
      "source": [
        "# 4. 평가 데이터셋 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PueVvBwDt8Yw"
      },
      "outputs": [],
      "source": [
        "source_train, source_valid, target_train, target_valid = train_test_split(source, target, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5TLa5CqthQ_",
        "outputId": "304117fe-a215-417f-fcd4-dbec66910ac8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(32, 14), dtype=tf.int32, name=None), TensorSpec(shape=(32, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "VOCAB_SIZE = tokenizer.num_words + 1 # <pad> 까지 포함. <unk>, <start>, <end>는 이미 num_words에 포함되어 있다.\n",
        "\n",
        "train = tf.data.Dataset.from_tensor_slices((source_train, target_train)) # 데이터 개수에 맞는 벡터들로 나눠진 source, target쌍의 tf.data.Dataset 생성. \n",
        "train = train.shuffle(len(source_train))\n",
        "train = train.batch(32, drop_remainder=True) # 배치 사이즈를 1에서 32로 변경.\n",
        "# valid = tf.data.Dataset.from_tensor_slices((source_valid, target_valid))\n",
        "\n",
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxu80isdqdRA"
      },
      "source": [
        "배치 사이즈를 최초 256으로 잡고 점점 낮췄을 때 val_loss도 같이 낮아졌다.  \n",
        "하지만 64에서 32로 낮췄을 때는 변화가 거의 없어서 64로 잡으려고 했는데  \n",
        "GPU 할당량이 부족해서 배치사이즈를 32로 낮추었다.  \n",
        "배치사이즈가 높을수록 학습이 빠르지만 그만큼 GPU를 많이 쓰고 예측 일반화 성능은 낮아질 수 있다.  \n",
        "다른 시각도 존재하는데 충분히 잘 만든 모델(거의 확정적으로 global minimum에 빠지는)은  \n",
        "충분한 리소스하에서는 최대한 많은 배치사이즈를 넣는 것이 배치 정규화의 효과를 더 잘 누릴 수 있다고 한다.\n",
        "\n",
        "일반적으로 배치사이즈는 32나 64가 가장 성능이 좋다고 알려져 있다.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq58u_qoqdRA"
      },
      "source": [
        "또한 val_loss를 구하기 위해 valid 변수를 생성할 때 헷갈리는 점들이 몇 개 있었다.  \n",
        "valid도 shuffle을 해야하는지, batch를 해야하는지 등을 찾아보았다.  \n",
        "하지만 이건 검증 혹은 평가의 개념을 잘 잡았다면 헷갈릴 일이 없는 것이였다.  \n",
        "애초에 valid는 학습하는 데이터가 아니기 때문에 계산만 1번 하면 되서  \n",
        "shuffle도 필요 없고 batch도 필요 없는 것이었다.\n",
        "\n",
        "하지만 결국 이것도 필요 없고 그냥 model.fit의 validation_data 파라미터에  \n",
        "(source_valid, target_valid) 형식으로 집어넣는 게 가장 일반적이고 간단한 방식이라는 것을 깨달았다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbObofi2MJc5"
      },
      "source": [
        "# 4. 인공지능 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "W_3N8QQ5MJc5"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(tf.keras.Model): # LSTM 모델 클래스 생성.\n",
        "    def __init__(self, vocab_size, embedding_size, lstm_size):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size) # 임베딩 레이어. 각 단어의 의미(분산 표현)를 업데이트.\n",
        "        self.lstm = tf.keras.layers.LSTM(lstm_size, return_sequences=True) # LSTM 레이어. Long Short-Term Memory. RNN에 장기 의존성을 더함.\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size) # 출력 레이어. 열의 길이가 vocab_size가 된다.\n",
        "        \n",
        "    def call(self, x): # 함수처럼 쓰면 predict를 할 수 있다. 또한 fit 메소드에 이 함수가 적용된다.\n",
        "        out = self.embedding(x)\n",
        "        out = self.lstm(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZQ5p84uqdRB"
      },
      "source": [
        "lstm 레이어를 2개 쌓았을 때보다 1개만 쌓은 모델이 더 성능이 좋게 나왔다.  \n",
        "물론 GPU 할당량이 더 있었다면 레이어를 2개를 쌓고 다양한 실험을 해봤을 것이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yZGbKlu4MJc5"
      },
      "outputs": [],
      "source": [
        "embedding_size = 2048\n",
        "lstm_size = 4096 # ★\n",
        "model = TextGenerator(VOCAB_SIZE, embedding_size, lstm_size) # 모델 생성."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoaFW0eVqdRB"
      },
      "source": [
        "embedding_size를 최초 256으로 잡고 점점 늘려갔을 때 val_loss가 낮아지는 것을 볼 수 있었다.  \n",
        "하지만 2048에서 4096으로 넘어갈 때는 성능차이가 거의 없었기 때문에 2048로 정했다.  \n",
        "lstm_size도 마찬가지로 최초 1024로 잡고 점점 늘려갔을 때 val_loss가 낮아졌는데  \n",
        "8196부터는 간혹 메모리 에러가 떠서 4096으로 잡았다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eAAy7zxsiVe",
        "outputId": "f8d3c33e-d32c-40e2-cb2e-165c5c5e59b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  24578048  \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  100679680 \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  49168097  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 174,425,825\n",
            "Trainable params: 174,425,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "for sample in train.take(1): break\n",
        "\n",
        "model(sample[0])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fReZwq_ZqdRB"
      },
      "source": [
        "총 퍼래머더 개수가 1억 7천만개가 나왔다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrAW1gFQMJc5"
      },
      "source": [
        "# 5. 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0blG0jZMJc6",
        "outputId": "6ae40332-ef55-4b43-d09c-bc3098bdc71c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3905/3905 [==============================] - 821s 210ms/step - loss: 2.8159 - val_loss: 2.4808\n",
            "Epoch 2/10\n",
            "3905/3905 [==============================] - 829s 212ms/step - loss: 2.1136 - val_loss: 2.2057\n",
            "Epoch 3/10\n",
            "3905/3905 [==============================] - 830s 212ms/step - loss: 1.5992 - val_loss: 2.1337\n",
            "Epoch 4/10\n",
            "3905/3905 [==============================] - 829s 212ms/step - loss: 1.2897 - val_loss: 2.1737\n",
            "Epoch 5/10\n",
            "3905/3905 [==============================] - 829s 212ms/step - loss: 1.1508 - val_loss: 2.2321\n",
            "Epoch 6/10\n",
            "3905/3905 [==============================] - 828s 212ms/step - loss: 1.0956 - val_loss: 2.2902\n",
            "Epoch 7/10\n",
            "3905/3905 [==============================] - 827s 212ms/step - loss: 1.0655 - val_loss: 2.3515\n",
            "Epoch 8/10\n",
            "3905/3905 [==============================] - 828s 212ms/step - loss: 1.0501 - val_loss: 2.3879\n",
            "Epoch 9/10\n",
            "3905/3905 [==============================] - 828s 212ms/step - loss: 1.0417 - val_loss: 2.4384\n",
            "Epoch 10/10\n",
            "3905/3905 [==============================] - 828s 212ms/step - loss: 1.0369 - val_loss: 2.4741\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faac5cd12d0>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam() # 최적화 도구를 adam으로 설정.\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none') # 교차 엔트로피 loss function.\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(train, epochs=10, validation_data=(source_valid, target_valid)) # epoch마다 검증을 하기 위해 (source_valid, target_valid) 형태로 val_data 넣기."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzTxrBkEqdRB"
      },
      "source": [
        "모델을 epoch 3까지 학습한 결과 val_loss가 2.2보다 낮은 2.13을을 달성했다. 따라서 epoch 3까지만 학습한 모델을 다시 생성한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_4vmXZ140O6",
        "outputId": "50c5ddf4-49de-47e1-e137-815e9152455a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "3905/3905 [==============================] - 876s 212ms/step - loss: 2.7881 - val_loss: 2.4614\n",
            "Epoch 2/3\n",
            "3905/3905 [==============================] - 830s 213ms/step - loss: 2.0652 - val_loss: 2.2202\n",
            "Epoch 3/3\n",
            "3905/3905 [==============================] - 831s 213ms/step - loss: 1.5655 - val_loss: 2.1559\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faad5f6a110>"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = TextGenerator(VOCAB_SIZE, embedding_size, lstm_size)\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "model.fit(train, epochs=3, validation_data=(source_valid, target_valid)) # epoch마다 검증을 하기 위해 (source_valid, target_valid) 형태로 val_data 넣기."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQYriSJrMJc6"
      },
      "source": [
        "# 6. 모델 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "d3gOFzgCMJc6"
      },
      "outputs": [],
      "source": [
        "# 첫 단어 혹은 구절를 입력하면 문장이 끝날 때 까지 뒤에 나올 단어를 하나씩 재귀적으로 예측해주는 함수를 생성.\n",
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence]) # 시퀀스 테스트 데이터 생성.\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64) # 텐서로 변환.\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    while True:\n",
        "        predict = model(test_tensor)\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] # predict 중 가장 큰 값을 가진 인덱스를 선택.\n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1) # 새로운 단어를 기존 텐서에다가 합치기.\n",
        "        if predict_word.numpy()[0] == end_token: break # predict_word가 <end>면 종료.\n",
        "        if test_tensor.shape[1] >= max_len: break # 최대 길이에 도달하면 강제 종료.\n",
        "\n",
        "    generated = \"\"\n",
        "    \n",
        "    for word_index in test_tensor[0].numpy(): # 텐서를 텍스트로 변환.\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "    \n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "M_bf-e21MJc7",
        "outputId": "b79cea17-8940-4724-a846-b7212227e51f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i love you for your love s mine , you re my true <end> '"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "'I love you for your love's mine, you're my true'  \n",
        "해석하면  \n",
        "'내 것인 너의 사랑을 위해 너를 사랑해, 너는 나의 진실이야'  \n",
        "뭔가 심오한 노랫말 같다. 외국에서는 이런 느낌의 노랫말이 많이 있는 것 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MZbCAjvRqdRC",
        "outputId": "25e532a5-5609-4a32-d04a-cf477223c3b0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> i m waiting for it , that green light , i want it <end> '"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> i\", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "'I'm waiting for it, that green light, I want it'  \n",
        "해석하면  \n",
        "'나는 그것을 기다리고 있다, 그린라이트, 나는 그것을 원한다'  \n",
        "그린라이트를 원하는 평범하고 일상적인 말이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3ZA5k0EcqdRC",
        "outputId": "fdad5df1-3087-4f70-ebcd-4e3681d54444"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> love me , hate me yeahh , say what you want about me <end> '"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> love\", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "'Love me, hate me yeahh, say what you want about me'  \n",
        "해석하면  \n",
        "'나를 사랑해줘, 나를 미워해줘, 나에게 원하는 게 무엇인지 말해줘'  \n",
        "나느 그대를 너무 사랑하는데 그대가 나를 대하는 마음은 너무나 부족해서  \n",
        "나를 사랑하지 않을 거면 차라리 미워라도 해달라는 의미인 것 같다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d874f726-6128-41ed-a972-4c4e763d1cc4",
        "tags": []
      },
      "source": [
        "# 6. 회고하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84703da4-024f-441e-8092-3995c327fdb6"
      },
      "source": [
        "## 6.1. 이번 프로젝트에서 어려웠던 점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kjWEeIHqdRD"
      },
      "source": [
        "![](GPU%20%EC%82%AC%EC%9A%A9%EB%9F%89%20%EC%A0%9C%ED%95%9C.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8sHp6PUqdRD"
      },
      "source": [
        "GPU가 역대급으로 많이 사용 됐던 프로젝트였다. 콜랩 GPU를 너무 많이 쓴 나머지 GPU 사용량이 제한됐다.  \n",
        "다양한 퍼래머더를 튜닝하기 위해 여러 세션을 동시에 돌려서 사용량이 많이 소모된 것이다.  \n",
        "콜랩 pro를 썼음에도 GPU를 마음껏 쓸 수 없다는 것에 배신감을 느꼈지만 기업 입장을 생각해보니 이해는 가더라.  \n",
        "차선책으로 부계정도 파봤지만 ip당 사용량을 할당하는지 부계정도 계속 제한되었다.  \n",
        "Microsoft Azure Machine Learning Studio도 가입해 봤는데 얼핏보니  \n",
        "GPU를 사용하는데 1시간에 8달러 정도길래 탈퇴를 알아보고 있다. 그래서 결국 LMS로 돌아오게 되었다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6141f6f1-d1a3-4e09-ba28-27729f313152"
      },
      "source": [
        "## 6.2. 프로젝트를 진행하면서 알아낸 점 혹은 아직 모호한 점"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3K1r2zqdRD"
      },
      "source": [
        "rnn 및 lstm이 내부적으로 어떻게 돌아가는지는 잘 모르겠다. 시간이 된다면 내부 프로세스 공식 좀 알아봐야겠다.  \n",
        "\n",
        "Tensor의 의미가 처음에는 3차원 이상의 자료구조를 부르는 건 줄 알았는데  \n",
        "(스칼라(0차원 배열) - 벡터(1차원 배열) - 매트릭스(2차원 배열) - 텐서(3차원 이상의 배열))  \n",
        "그냥 \"자료구조\" 그 자체라는 것을 알았다.  \n",
        "(스칼라(0차원 텐서) - 벡터(1차원 텐서) - 매트릭스(2차원 텐서) - 3차원 이상의 텐서)  \n",
        "단 2차원 이하의 텐서는 따로 부르는 명칭이 있으므로 2차원 이하에서는 텐서라고 부르는 건  \n",
        "오히려 헷갈리는 것 같다. tokenize 함수를 정의할 때  \n",
        "tokenizer.texts_to_sequences(corpus)의 return을 받는 변수명을 tensor로 지으셔서  \n",
        "많이 헷갈렸다. 구글링의 다른 사람들은 변수명을 sequences로 지었다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba3637ac-2680-4892-8945-00f993730099"
      },
      "source": [
        "## 6.3. 루브릭 평가 지표를 맞추기 위해 시도한 것들"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F07afh2NqdRD"
      },
      "source": [
        "처음에 거의 뭐 2.8점 정도밖에 점수가 안나와서 '아... 점수 맞추기 힘들겠구나' 라는 생각을 했다.  \n",
        "토큰화 했을 때 토큰의 개수가 15개 이상인 것들을 지웠고,  \n",
        "Embedding size와 Hidden size를 조절했고, batch size를 조절했다.  \n",
        "다만 Embedding size와 Hidden size 조절하는데 gpu 할당량을 다써서  \n",
        "vocab size는 조절해보지 못한게 아쉽다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0fa0bdb-f362-428b-af91-df653b3d4632"
      },
      "source": [
        "## 6.4. 만약에 루브릭 평가 관련 지표를 달성 하지 못했을 때, 이유에 관한 추정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCSmU5FNqdRD"
      },
      "source": [
        "다행히 이번에도 평가 지표를 달성했지만 만약 gpu가 전혀 없는 환경이었다면 절대로 달성하지 못했을 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5acff079-52f7-499f-a3bf-4ebb5927efa7"
      },
      "source": [
        "## 6.5. 회상 혹은 자기 다짐"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBLIOiboKWsS"
      },
      "source": [
        "구글 콜랩은 GPU 할당량이 정해져 있어서 파일을 마구잡이로 실행하다보면  \n",
        "6시간 정도 밖에 쓰지 못하고 12시간 후에 다시 할당량이 충전된다.  \n",
        "따라서 함부로 실행버튼을 누르기 보다는 지금 하고자 하는 실험을  \n",
        "좀 더 명확하게 규정하고, 기록하고, 정리한 후에 실행해야겠다는 생각을 하게 되었다.\n",
        "\n",
        "NLP가 조금 재밌어 지려고 하는 차에 GPU 부족에 막혀서 매우 아쉬웠다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofcL5TPDqdRE"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "e76908176805cd1f43d0077959698ad43a5b10a4015d2b23b864ac3da32cb183"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
